# ğŸ¤– N8N + Ollama/LM Studio Integration

## ğŸ”§ Ãšprava workflow pro lokÃ¡lnÃ­ LLM

### 1. **NahraÄte OpenAI node za HTTP Request node**

V N8N workflow:
1. SmaÅ¾te **"AI Deep Document Analysis"** node
2. PÅ™idejte novÃ½ **HTTP Request** node
3. Importujte nastavenÃ­ z `n8n_ollama_document_node.json`

### 2. **Konfigurace pro Ollama**

```json
{
  "method": "POST",
  "url": "http://host.docker.internal:11434/api/generate",
  "headers": {
    "Content-Type": "application/json"
  },
  "body": {
    "model": "llama3.3:70b",  // nebo jinÃ½ model
    "prompt": "...",
    "stream": false,
    "temperature": 0.1
  }
}
```

### 3. **Konfigurace pro LM Studio**

```json
{
  "method": "POST", 
  "url": "http://host.docker.internal:1234/v1/chat/completions",
  "headers": {
    "Content-Type": "application/json"
  },
  "body": {
    "model": "loaded-model",
    "messages": [
      {
        "role": "system",
        "content": "Jsi expert na analÃ½zu dokumentÅ¯..."
      },
      {
        "role": "user", 
        "content": "{{ $json.emailText }}"
      }
    ],
    "temperature": 0.1,
    "max_tokens": 1000
  }
}
```

## ğŸ“Š DoporuÄenÃ© modely

### Pro rychlost (7-14B):
- `mistral:7b-instruct` - RychlÃ½, dobrÃ¡ ÄeÅ¡tina
- `qwen2.5:14b` - VÃ½bornÃ½ pomÄ›r vÃ½kon/rychlost
- `czech-finance:latest` - SpecializovanÃ½ na finance

### Pro pÅ™esnost (32-70B):
- `llama3.3:70b` - NejlepÅ¡Ã­ pÅ™esnost
- `qwen2.5:32b` - DobrÃ½ kompromis
- `mixtral:8x7b` - MoE architektura

## ğŸ”„ ZpracovÃ¡nÃ­ odpovÄ›di

Ollama vracÃ­ JSON s polem `response`:

```javascript
// V nÃ¡sledujÃ­cÃ­m Code node pÅ™idejte:
const ollamaResponse = JSON.parse($json.response);
const analysis = JSON.parse(ollamaResponse.response);
```

## âš¡ Optimalizace vÃ½konu

### 1. **PouÅ¾ijte menÅ¡Ã­ model pro pre-filtering**
- PrvnÃ­ prÅ¯chod: `mistral:7b` (rychlÃ© tÅ™Ã­dÄ›nÃ­)
- DruhÃ½ prÅ¯chod: `llama3.3:70b` (pÅ™esnÃ¡ analÃ½za)

### 2. **CachovÃ¡nÃ­**
- UklÃ¡dejte vÃ½sledky klasifikace
- Znovu neanalyzujte stejnÃ© dokumenty

### 3. **Batch processing**
- ZpracovÃ¡vejte vÃ­ce dokumentÅ¯ najednou
- VyuÅ¾ijte paralelnÃ­ bÄ›h

## ğŸ§ª TestovÃ¡nÃ­

### Test Ollama API:
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.3:70b",
  "prompt": "Analyzuj: Faktura Ä. 2024001, IÄŒO: 12345678",
  "stream": false
}'
```

### Test LM Studio API:
```bash
curl http://localhost:1234/v1/models
```

## ğŸ“ Prompt Engineering

### StrukturovanÃ½ prompt:
```
ÃšKOL: Klasifikuj dokument

KONTEXT:
- NÃ¡zev: {filename}
- Text: {content}

TYPY:
1. FAKTURA - mÃ¡ IÄŒO, DIÄŒ, ÄÃ¡stku
2. PROFORMA - nenÃ­ daÅˆovÃ½ doklad
...

VÃSTUP: JSON
{
  "documentType": "...",
  "confidence": 0.9,
  ...
}
```

### ÄŒeskÃ© vs. anglickÃ© prompty:
- ÄŒeskÃ© modely: PouÅ¾ijte ÄeskÃ½ prompt
- GlobÃ¡lnÃ­ modely: Mix ÄeÅ¡tiny a angliÄtiny funguje

## ğŸ” BezpeÄnost

1. **LokÃ¡lnÃ­ zpracovÃ¡nÃ­** - data neopouÅ¡tÃ­ vÃ¡Å¡ server
2. **Å½Ã¡dnÃ© API klÃ­Äe** - nenÃ­ tÅ™eba platit za tokeny
3. **PlnÃ¡ kontrola** - mÅ¯Å¾ete upravit modely

## ğŸš€ VÃ½hody lokÃ¡lnÃ­ch LLM

âœ… **Zdarma** - Å¾Ã¡dnÃ© poplatky za API  
âœ… **Rychlost** - pÅ™i dobrÃ©m HW rychlejÅ¡Ã­ neÅ¾ cloud  
âœ… **SoukromÃ­** - data zÅ¯stÃ¡vajÃ­ lokÃ¡lnÄ›  
âœ… **Offline** - funguje bez internetu  
âœ… **Customizace** - fine-tuning na vaÅ¡e dokumenty